{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6788e33f-baf1-4847-a993-a9ec85f7d7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /Users/perseus/anaconda3/lib/python3.11/site-packages (3.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from jiwer) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#SPEECH TO TEXT\n",
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55f00840-99c5-4609-824c-9f0694b759c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/perseus/anaconda3/lib/python3.11/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/perseus/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "##importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from jiwer import wer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a92e93f1-f98b-4f92-bcf7-7501fc0fd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "## extracting the data of LJspeech dataset using the keras in tensorflow\n",
    "## untar = True is used to extract the file if it is archived\n",
    "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19fc1209-1c52-4acf-8df5-58e7ae65ce58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LJ001-0001</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LJ001-0002</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LJ001-0003</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LJ001-0004</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LJ001-0005</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LJ001-0006</td>\n",
       "      <td>And it is worth mention in passing that, as an...</td>\n",
       "      <td>And it is worth mention in passing that, as an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LJ001-0007</td>\n",
       "      <td>the earliest book printed with movable types, ...</td>\n",
       "      <td>the earliest book printed with movable types, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LJ001-0008</td>\n",
       "      <td>has never been surpassed.</td>\n",
       "      <td>has never been surpassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LJ001-0009</td>\n",
       "      <td>Printing, then, for our purpose, may be consid...</td>\n",
       "      <td>Printing, then, for our purpose, may be consid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LJ001-0010</td>\n",
       "      <td>Now, as all books not primarily intended as pi...</td>\n",
       "      <td>Now, as all books not primarily intended as pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                                  1  \\\n",
       "0  LJ001-0001  Printing, in the only sense with which we are ...   \n",
       "1  LJ001-0002                     in being comparatively modern.   \n",
       "2  LJ001-0003  For although the Chinese took impressions from...   \n",
       "3  LJ001-0004  produced the block books, which were the immed...   \n",
       "4  LJ001-0005  the invention of movable metal letters in the ...   \n",
       "5  LJ001-0006  And it is worth mention in passing that, as an...   \n",
       "6  LJ001-0007  the earliest book printed with movable types, ...   \n",
       "7  LJ001-0008                          has never been surpassed.   \n",
       "8  LJ001-0009  Printing, then, for our purpose, may be consid...   \n",
       "9  LJ001-0010  Now, as all books not primarily intended as pi...   \n",
       "\n",
       "                                                   2  \n",
       "0  Printing, in the only sense with which we are ...  \n",
       "1                     in being comparatively modern.  \n",
       "2  For although the Chinese took impressions from...  \n",
       "3  produced the block books, which were the immed...  \n",
       "4  the invention of movable metal letters in the ...  \n",
       "5  And it is worth mention in passing that, as an...  \n",
       "6  the earliest book printed with movable types, ...  \n",
       "7                          has never been surpassed.  \n",
       "8  Printing, then, for our purpose, may be consid...  \n",
       "9  Now, as all books not primarily intended as pi...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavs_path = data_path + \"/wavs/\"\n",
    "metadata_path = data_path + \"/metadata.csv\"\n",
    "##converting the csv file to a dataframe using pandas\n",
    "metadata_df = pd.read_csv(metadata_path, sep = \"|\", header = None, quoting = 3)\n",
    "metadata_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc05abf9-83d5-4d61-b005-41ac66046496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>normalized transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LJ006-0032</td>\n",
       "      <td>Messrs. Crawford and Russell proceeded to carr...</td>\n",
       "      <td>Messrs. Crawford and Russell proceeded to carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LJ021-0018</td>\n",
       "      <td>the tremendous power of organization has combi...</td>\n",
       "      <td>the tremendous power of organization has combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LJ016-0357</td>\n",
       "      <td>No time was lost in carrying out the dread cer...</td>\n",
       "      <td>No time was lost in carrying out the dread cer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file name                                      transcription  \\\n",
       "0  LJ006-0032  Messrs. Crawford and Russell proceeded to carr...   \n",
       "1  LJ021-0018  the tremendous power of organization has combi...   \n",
       "2  LJ016-0357  No time was lost in carrying out the dread cer...   \n",
       "\n",
       "                            normalized transcription  \n",
       "0  Messrs. Crawford and Russell proceeded to carr...  \n",
       "1  the tremendous power of organization has combi...  \n",
       "2  No time was lost in carrying out the dread cer...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metadata_df.columns = [\"file name\", \"transcription\", \"normalized transcription\"]\n",
    "##reshuffling the rows of the metadata df in a random order and not not\n",
    "\n",
    "metadata_df = metadata_df.sample(frac = 1).reset_index (drop = True)\n",
    "metadata_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf7e15c4-a9b8-4646-b04f-98cf429372f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the training dataframe :  {131}\n",
      "size of the test dataframe :  {12969}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## splitting the dataframe into two parts : training (90%), test (10%); using int to get an index at which to split\n",
    "split = int(len(metadata_df) *0.010)\n",
    "df_train = metadata_df [:split]\n",
    "df_test = metadata_df[split:]\n",
    "print (\"size of the training dataframe : \",{len(df_train)})\n",
    "print (\"size of the test dataframe : \",{len(df_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa621121-7b40-45ad-90d2-4b500e08683e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "#defining a list of allowed vocabulary\n",
    "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!\"]\n",
    "#converting the charactrs to integer values using keras; any character not in the list is given an empty string\n",
    "char_to_num = keras.layers.StringLookup(vocabulary = characters, oov_token = \"\")\n",
    "#converting the integer back to the character using keras, specifying it using invert\n",
    "num_to_char = keras.layers.StringLookup(vocabulary = char_to_num.get_vocabulary(), oov_token = \"\", invert = True)\n",
    "print (char_to_num.get_vocabulary())\n",
    "print(char_to_num.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b7b4e00-1358-4fd0-b8aa-38a808ed56ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 ms ± 1.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# setting the frame length, frame step and forward fourier transform rate\n",
    "frame_length = 256\n",
    "frame_step = 160\n",
    "fft_length = 384\n",
    "#defining function for getting a spectrogram of the audio files and its label\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def encode_single_sample (wav_file, label):\n",
    "  #reading the audio file\n",
    "  file = tf.io.read_file (wavs_path + wav_file + \".wav\")\n",
    "  #decoding the audio file using audio.decode_wav from the TensorFlow package to a float tensor\n",
    "  audio,_ = tf.audio.decode_wav(file)\n",
    "  #removing dimensions of size 1 from the tensor\n",
    "  audio = tf.squeeze(audio, axis = -1)\n",
    "  #changing the data type\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  #converting the audio signal into a time frequency representation using Short Time Fourier Transfrom\n",
    "  spectrogram = tf.signal.stft (audio, frame_length = frame_length, frame_step = frame_step, fft_length = fft_length)\n",
    "  #calculating the magnitude of the spectrogram\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  #normalizing the power\n",
    "  spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "  #performing the standardization of the spectrogram\n",
    "  means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "  stddev = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "  spectrogram = (spectrogram - means)/(stddev - 1e-10)\n",
    "  #splitting the label character and converting it to a numerical representation\n",
    "  label = tf.strings.lower(label)\n",
    "  label = tf.strings.unicode_split(label, input_encoding = \"UTF-8\")\n",
    "  label = char_to_num(label)\n",
    "  return spectrogram, label\n",
    "batch_size = 32\n",
    "file_names = np.array(df_train[\"file name\"])\n",
    "transcriptions = np.array(df_train[\"normalized transcription\"])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(( file_names,transcriptions))\n",
    "train_dataset = (train_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((file_names, transcriptions))\n",
    "test_dataset = (test_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df676ef-2ffc-4177-8a93-2610de6ae3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining function for CTCloss; y_true--> target , y_pred--> predicted \n",
    "def CTCloss (y_true, y_pred):\n",
    "  #getting the shape of the tensor and extracting the first dimension\n",
    "  batch_len = tf.cast(tf.shape(y_true)[0], dtype = \"int64\")\n",
    "  #calculating the second dimension  \n",
    "  input_length = tf.cast(tf.shape(y_pred)[1], dtype = \"int64\")\n",
    "   #calculating the length of the label sequence\n",
    "  label_length = tf.cast(tf.shape(y_true)[1], dtype = \"int64\")\n",
    "   #giving it shape (batch_len, 1), creating a 2D tensor with ones \n",
    "  input_length = input_length * tf.ones(shape = (batch_len, 1), dtype = \"int64\")\n",
    "  label_length = label_length * tf.ones(shape = (batch_len, 1), dtype = \"int64\")\n",
    "  #calculating the CTC loss\n",
    "  #keras is deeplearning framework(API) in the TensorFlow package \n",
    "  #the backend function does the mathematical operations on the tensors  \n",
    "  loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afe60017-5472-4984-a685-0724665ed868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from custom_loss_functions import CTCloss\n",
    "\n",
    "# Register the custom loss function\n",
    "with custom_object_scope({\"CTCloss\": CTCloss}):\n",
    "    loaded_model = load_model(\"stt_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39455e5a-0393-4b65-a9ef-d6dd111cd10c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Deepspeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 193)]                         0                \n",
      "                                                                                                              \n",
      " expanddim (Reshape)                             (None, None, 193, 1)                        0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 97, 32)                        14432            \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 97, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 97, 32)                        0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 49, 32)                        236544           \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 49, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 49, 32)                        0                \n",
      "                                                                                                              \n",
      " reshape_8 (Reshape)                             (None, None, 1568)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 1024)                          6395904          \n",
      "                                                                                                              \n",
      " dropout_40 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_2 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_41 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_3 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_42 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_4 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_43 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_5 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 1024)                          1049600          \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dropout_44 (Dropout)                            (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dense_8 (Dense)                                 (None, None, 31)                            31775            \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 26627455 (101.58 MB)\n",
      "Trainable params: 26627327 (101.58 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def draft_model (inputdim, outputdim, rnn_layers = 5, rnn_units = 128):\n",
    "  input_spectrogram = layers.Input ((None, inputdim), name = \"input\")\n",
    "  x = layers.Reshape((-1, inputdim, 1), name = \"expanddim\")(input_spectrogram)\n",
    "  x = layers.Conv2D( filters = 32, kernel_size = [11,41], strides = [2,2], padding = \"same\", use_bias = False, name = \"conv_1\",)(x)\n",
    "  x = layers.BatchNormalization(name = \"conv_1_bn\")(x)\n",
    "  x = layers.ReLU(name = \"conv_1_relu\")(x)\n",
    "  x = layers.Conv2D(filters = 32, kernel_size =[11,21], strides = [1,2], padding = \"same\", use_bias = False, name = \"conv_2\")(x)\n",
    "  x = layers.BatchNormalization(name =\"conv_2_bn\")(x)\n",
    "  x = layers.ReLU(name = \"conv_2_relu\")(x)\n",
    "  x = layers.Reshape((-1, x.shape[-2]*x.shape[-1]))(x)\n",
    "  for i in range (1, rnn_layers + 1):\n",
    "    recurrent = layers.GRU(units= rnn_units, activation = \"tanh\", recurrent_activation = \"sigmoid\", use_bias = True, return_sequences = True,\n",
    "                           reset_after = True, name = f\"gru_{i}\",)\n",
    "    x = layers.Bidirectional(recurrent, name = f\"bidirectional_{i}\", merge_mode=\"concat\")(x)\n",
    "    if i < rnn_layers:\n",
    "      x = layers.Dropout(rate= 0.5)(x)\n",
    "  x = layers.Dense(units = rnn_units * 2, name = \"dense_1\")(x)\n",
    "  x = layers.ReLU(name = \"dense_1_relu\")(x)\n",
    "  x = layers.Dropout ( rate = 0.5 )(x)\n",
    "  output = layers.Dense(units = outputdim + 1, activation = \"softmax\")(x)\n",
    "  model = keras.Model(input_spectrogram, output, name = \"Deepspeech_2\")\n",
    "  opt = Adam(learning_rate= 1e-4) \n",
    "  model.compile(optimizer = opt, loss= CTCloss)\n",
    "  return model\n",
    "fft_length = 384\n",
    "model = draft_model(inputdim = fft_length // 2 + 1, outputdim= char_to_num.vocabulary_size(), rnn_units = 512,)\n",
    "model.summary(line_length = 110)\n",
    "def decode_batch_predictions (pred):\n",
    "  input_len = np.ones(pred.shape[0]) * pred.shape [1]\n",
    "  results = keras.backend.ctc_decode(pred, input_length = input_len, greedy = True)[0][0]\n",
    "  output_text =[]\n",
    "  for result in results :\n",
    "    result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "    output_text.append(result)\n",
    "  return output_text\n",
    "\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, dataset):\n",
    "    super().__init__()\n",
    "    self.dataset = dataset\n",
    "  def on_epoch_end(self, epoch : int, logs = None):\n",
    "     predictions = []\n",
    "     targets = []\n",
    "     for batch in self.dataset:\n",
    "      X, y = batch\n",
    "      batch_predictions = model.predict(X)\n",
    "      batch_predictions = decode_batch_predictions (batch_predictions)\n",
    "      predictions.extend(batch_predictions)\n",
    "      for label in y :\n",
    "       label = (tf.strings.reduce_join (num_to_char(label)).numpy().decode(\"utf-8\"))\n",
    "       targets.append (label)\n",
    "     wer_score = wer(targets,predictions)\n",
    "     print (\".\" *100)\n",
    "     print (f\"word error rate: , {wer_score:.4f}\")\n",
    "     print (\".\" *100)\n",
    "     for i in np.random.randint (0, len(predictions),2):\n",
    "      print (\"target : \", (targets[i]))\n",
    "      print (\"prediction: \", (predictions[i]))\n",
    "      print (\".\" *100)\n",
    "frame_length = 256\n",
    "frame_step = 160\n",
    "fft_length = 384\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def encode_single_sample (wav_file, label):\n",
    "  #reading the audio file\n",
    "  file = tf.io.read_file (wavs_path + wav_file + \".wav\")\n",
    "  #decoding the audio file using audio.decode_wav from the TensorFlow package to a float tensor\n",
    "  audio,_ = tf.audio.decode_wav(file)\n",
    "  #removing dimensions of size 1 from the tensor\n",
    "  audio = tf.squeeze(audio, axis = -1)\n",
    "  #changing the data type\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  #converting the audio signal into a time frequency representation using Short Time Fourier Transfrom\n",
    "  spectrogram = tf.signal.stft (audio, frame_length = frame_length, frame_step = frame_step, fft_length = fft_length)\n",
    "  #calculating the magnitude of the spectrogram\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  #normalizing the power\n",
    "  spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "  #performing the standardization of the spectrogram\n",
    "  means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "  stddev = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "  spectrogram = (spectrogram - means)/(stddev - 1e-10)\n",
    "  #splitting the label character and converting it to a numerical representation\n",
    "  label = tf.strings.lower(label)\n",
    "  label = tf.strings.unicode_split(label, input_encoding = \"UTF-8\")\n",
    "  label = char_to_num(label)\n",
    "  return spectrogram, label\n",
    "batch_size = 32\n",
    "file_names = np.array(df_train[\"file name\"])\n",
    "transcriptions = np.array(df_train[\"normalized transcription\"])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(( file_names,transcriptions))\n",
    "train_dataset = (train_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((file_names, transcriptions))\n",
    "test_dataset = (test_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n",
    "epochs = 1\n",
    "validation_callback = CallbackEval(test_dataset)\n",
    "history = model.fit(train_dataset, validation_data= test_dataset, epochs= epochs, callbacks = [validation_callback],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8680baef-7d85-4f4e-b06f-e647d7895ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save ('Downloads/stt_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91760d-dc35-49f7-9689-faef35894a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draft_model (inputdim, outputdim, rnn_layers = 5, rnn_units = 128):\n",
    "  input_spectrogram = layers.Input ((None, inputdim), name = \"input\")\n",
    "  x = layers.Reshape((-1, inputdim, 1), name = \"expanddim\")(input_spectrogram)\n",
    "  x = layers.Conv2D( filters = 32, kernel_size = [11,41], strides = [2,2], padding = \"same\", use_bias = False, name = \"conv_1\",)(x)\n",
    "  x = layers.BatchNormalization(name = \"conv_1_bn\")(x)\n",
    "  x = layers.ReLU(name = \"conv_1_relu\")(x)\n",
    "  x = layers.Conv2D(filters = 32, kernel_size =[11,21], strides = [1,2], padding = \"same\", use_bias = False, name = \"conv_2\")(x)\n",
    "  x = layers.BatchNormalization(name =\"conv_2_bn\")(x)\n",
    "  x = layers.ReLU(name = \"conv_2_relu\")(x)\n",
    "  x = layers.Reshape((-1, x.shape[-2]*x.shape[-1]))(x)\n",
    "  for i in range (1, rnn_layers + 1):\n",
    "    recurrent = layers.GRU(units= rnn_units, activation = \"tanh\", recurrent_activation = \"sigmoid\", use_bias = True, return_sequences = True,\n",
    "                           reset_after = True, name = f\"gru_{i}\",)\n",
    "    x = layers.Bidirectional(recurrent, name = f\"bidirectional_{i}\", merge_mode=\"concat\")(x)\n",
    "    if i < rnn_layers:\n",
    "      x = layers.Dropout(rate= 0.5)(x)\n",
    "  x = layers.Dense(units = rnn_units * 2, name = \"dense_1\")(x)\n",
    "  x = layers.ReLU(name = \"dense_1_relu\")(x)\n",
    "  x = layers.Dropout ( rate = 0.5 )(x)\n",
    "  output = layers.Dense(units = outputdim + 1, activation = \"softmax\")(x)\n",
    "  model = keras.Model(input_spectrogram, output, name = \"Deepspeech_2\")\n",
    "  opt = tf.keras.optimizers.legacy.Adam(learning_rate= 1e-4)\n",
    "  model.compile(optimizer = opt, loss= CTCloss)\n",
    "  return model\n",
    "fft_length = 384\n",
    "model = draft_model(inputdim = fft_length // 2 + 1, outputdim= char_to_num.vocabulary_size(), rnn_units = 512,)\n",
    "model.summary(line_length = 110)\n",
    "def decode_batch_predictions (pred):\n",
    "  input_len = np.ones(pred.shape[0]) * pred.shape [1]\n",
    "  results = keras.backend.ctc_decode(pred, input_length = input_len, greedy = True)[0][0]\n",
    "  output_text =[]\n",
    "  for result in results :\n",
    "    result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "    output_text.append(result)\n",
    "  return output_text\n",
    "\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, dataset):\n",
    "    super().__init__()\n",
    "    self.dataset = dataset\n",
    "  def on_epoch_end(self, epoch : int, logs = None):\n",
    "     predictions = []\n",
    "     targets = []\n",
    "     for batch in self.dataset:\n",
    "      X, y = batch\n",
    "      batch_predictions = model.predict(X)\n",
    "      batch_predictions = decode_batch_predictions (batch_predictions)\n",
    "      predictions.extend(batch_predictions)\n",
    "      for label in y :\n",
    "       label = (tf.strings.reduce_join (num_to_char(label)).numpy().decode(\"utf-8\"))\n",
    "       targets.append (label)\n",
    "     wer_score = wer(targets,predictions)\n",
    "     print (\".\" *100)\n",
    "     print (f\"word error rate: , {wer_score:.4f}\")\n",
    "     print (\".\" *100)\n",
    "     for i in np.random.randint (0, len(predictions),2):\n",
    "      print (\"target : \", (targets[i]))\n",
    "      print (\"prediction: \", (predictions[i]))\n",
    "      print (\".\" *100)\n",
    "frame_length = 256\n",
    "frame_step = 160\n",
    "fft_length = 384\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def encode_single_sample (wav_file, label):\n",
    "  #reading the audio file\n",
    "  file = tf.io.read_file (wavs_path + wav_file + \".wav\")\n",
    "  #decoding the audio file using audio.decode_wav from the TensorFlow package to a float tensor\n",
    "  audio,_ = tf.audio.decode_wav(file)\n",
    "  #removing dimensions of size 1 from the tensor\n",
    "  audio = tf.squeeze(audio, axis = -1)\n",
    "  #changing the data type\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  #converting the audio signal into a time frequency representation using Short Time Fourier Transfrom\n",
    "  spectrogram = tf.signal.stft (audio, frame_length = frame_length, frame_step = frame_step, fft_length = fft_length)\n",
    "  #calculating the magnitude of the spectrogram\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  #normalizing the power\n",
    "  spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "  #performing the standardization of the spectrogram\n",
    "  means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "  stddev = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "  spectrogram = (spectrogram - means)/(stddev - 1e-10)\n",
    "  #splitting the label character and converting it to a numerical representation\n",
    "  label = tf.strings.lower(label)\n",
    "  label = tf.strings.unicode_split(label, input_encoding = \"UTF-8\")\n",
    "  label = char_to_num(label)\n",
    "  return spectrogram, label\n",
    "batch_size = 32\n",
    "file_names = np.array(df_train[\"file name\"])\n",
    "transcriptions = np.array(df_train[\"normalized transcription\"])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(( file_names,transcriptions))\n",
    "train_dataset = (train_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((file_names, transcriptions))\n",
    "test_dataset = (test_dataset.map(encode_single_sample, num_parallel_calls = tf.data.AUTOTUNE).padded_batch(batch_size).prefetch(buffer_size = tf.data.AUTOTUNE))\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "for batch in test_dataset :\n",
    "  X, y = batch\n",
    "  batch_predictions = model.predict(X)\n",
    "  batch_predictions = decode_batch_predictions (batch_predictions)\n",
    "  predictions.extend(batch_predictions)\n",
    "  for label in y :\n",
    "       label = (tf.strings.reduce_join (num_to_char(label)).numpy().decode(\"utf-8\"))\n",
    "       targets.append (label)\n",
    "wer_score = wer(targets,predictions)\n",
    "print (\".\" *100)\n",
    "print (f\"word error rate: , {wer_score:.4f}\")\n",
    "print (\".\" *100)\n",
    "for i in np.random.randint (0, len(predictions),2):\n",
    "      print (\"target : \", (targets[i]))\n",
    "      print (\"prediction: \", (predictions[i]))\n",
    "      print (\".\" *100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cb8fd52-13a8-4534-be6f-37a33990480e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Campbell Biology, Tenth Edition - Reece, Urry, Cain et al.pdf', 'CV_VatsalKabra.pdf', '(Undergraduate texts in mathematics) Axler, Sheldon - Linear Algebra Done Right-Springer International Publishing _ Imprint_ Springer (2015).pdf', 'homologous recombination.pdf', '.DS_Store', '.localized', 'linear algebra ciarlet.pdf', '20191205_164123.jpg', 'custom_loss.ipynb', 'books', 'idc 409', 'audio', 'internship', 'stt_model', '.ipynb_checkpoints', 'bio 5th sem', 'cv.VatsalKabra.pdf', 'idc409_2nd_draft.ipynb', 'Summer Internship Portals.pdf', 'custom_loss_functions.py', 'CV VatsalKabra.pdf', 'cv.pdf']\n",
      "The file 'custom_loss_functions.py' is in the current directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# List the files in the current directory\n",
    "files_in_directory = os.listdir(current_directory)\n",
    "print (files_in_directory)\n",
    "# Check if 'custom_loss_functions.py' is in the list of files\n",
    "if 'custom_loss_functions.py' in files_in_directory:\n",
    "    print(\"The file 'custom_loss_functions.py' is in the current directory.\")\n",
    "else:\n",
    "    print(\"The file 'custom_loss_functions.py' is not in the current directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888bb27-6b75-41e1-9a71-4f5ffa6f0b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
